<!DOCTYPE HTML>
<!--
  Berkeley Vision and Learning Center (BVLC)

  Design based on:
  Strongly Typed 1.0 by HTML5 UP
  html5up.net | @n33co
  Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <title>Berkeley Vision and Learning Center</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta name="viewport" content="width=1040" />
    <base href="../" />
    <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600|Arvo:700" rel="stylesheet" type="text/css" />
    <!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
    <script src="js/jquery.min.js"></script>
    <script src="js/jquery.dropotron.js"></script>
    <script src="js/config.js"></script>
    <script src="js/skel.min.js"></script>
    <script src="js/skel-panels.min.js"></script>
    <noscript>
      <link rel="stylesheet" href="css/skel-noscript.css" />
      <link rel="stylesheet" href="css/style.css" />
      <link rel="stylesheet" href="css/style-desktop.css" />
    </noscript>
  </head>
  <body class="homepage">

    <!-- Header Wrapper -->
      <div id="header-wrapper">

        <!-- Header -->
          <div id="header" class="container">

            <!-- Logo -->
            <h1 id="logo">
              <a id="home" href="#">Berkeley Vision and Learning Center</a>
            </h1>
            <p>
              A new center for focused collaborations with industry
              <br>
              on autonomous perception research.
            </p>

            <!-- Nav -->
            <nav id="nav">
              <ul>
                <li><a href="#header" class="icon icon-home"><span>BVLC</span></a></li>
                <li>
                  <a href="#header" class="icon icon-cogs"><span>Members</span></a>
                  <ul>
                    <li><a href="#pi">Principal Investigators</a></li>
                    <li><a href="#sponsor">Sponsors</a></li>
                  </ul>
                </li>
                <li><a href="#research" class="icon icon-bar-chart"><span>Research</span></a></li>
                <li><a href="#fellowships" class="icon icon-book"><span>Fellowships</span></a></li>
                <li><a href="#events" class="icon icon-calendar"><span>Events</span></a></li>
              </ul>
            </nav>

          </div>

      </div>

      <div class="features-wrapper">
        <section class="features projects container">

          <h2 id="projects">BVLC Spring Retreat '14 Slides &amp; Posters</h2>
          <ul>
            <li>
              <a href="https://drive.google.com/folderview?id=0ByigZJZpjiQgZHQxTUczdlEzbFk&usp=sharing">Slides</a>
            </li>
            <li>
              <a href="https://drive.google.com/folderview?id=0ByigZJZpjiQgd3o0a294VWtsMms&usp=sharing">Posters</a>
            </li>
          </ul>
        </section>
      </div>
      <div class="features-wrapper">
        <section class="features projects container">

          <h2 id="projects">BVLC Project Highlights</h2>
          <p>
            These highlights represent key current research at the Berkeley
            Vision and Learning Center and offer promising directions and
            potential sponsor collaborations.
          </p>

          <div class="row">
            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://rll.berkeley.edu/isrr2013lfd/" class="image"><img
                    src="images/projects/abbeel/1-LfD.png" alt="Learning from
                    demonstrations through non-rigid registration" /><span>Learning from demonstrations through non-rigid registration</span></a>
                </div>
              </div>
              <p>
We consider the problem of teaching robots by demonstration how to perform
manipulation tasks, in which the geometry (including size, shape, and pose) of
the relevant objects varies from trial to trial. We are developing a method,
which we call trajectory transfer, for adapting a demonstrated trajectory from
the geometry at training time to the geometry at test time. Trajectory transfer
is based on non-rigid registration, which computes a smooth transformation from
the training scene onto the testing scene. We then perform a multi-step task by
repeatedly looking up the nearest demonstration and then applying trajectory
transfer. We have preliminary experimental results in which we enable a PR2
robot to autonomously tie five different types of knots in rope.
<a href="http://rll.berkeley.edu/isrr2013lfd/">Details</a> / <a href="http://rll.berkeley.edu/rapprentice/">Code</a>
              </p>
            </div>

            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://vis.berkeley.edu/papers/vpuppet/" class="image"><img
                    src="images/projects/agrawala/vpuppet.png" alt="Video
                    Puppetry: A Performative Interface for Cutout Animation"
                    /><span>Video Puppetry: A Performative Interface for Cutout Animation</span></a>
                </div>
              </div>
              <p>
We present a video-based interface that allows users of all skill levels to
quickly create cutout-style animations by performing the character motions. The
puppeteer first creates a cast of physical puppets using paper, markers and
scissors. He then physically moves these puppets to tell a story. Using an
inexpensive overhead camera our system tracks the motions of the puppets and
renders them on a new background while removing the puppeteer's hands. Our
system runs in real-time (at 30 fps) so that the puppeteer and the audience can
immediately see the animation that is created. Our system also supports a
variety of constraints and effects including articulated characters, multi-track
animation, scene changes, camera controls, 21/2-D environments, shadows, and
animation cycles. Users have evaluated our system both quantitatively and
qualitatively: In tests of low-level dexterity, our system has similar accuracy
to a mouse interface. For simple story telling, users prefer our system over
either a mouse interface or traditional puppetry. We demonstrate that even
first-time users, including an eleven-year-old, can use our system to quickly
turn an original story idea into an animation.
<a href="http://vis.berkeley.edu/papers/vpuppet/">Details</a>
              </p>
            </div>
          </div>

          <div class="row">
            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://tele-immersion.citris-uc.org/berkeley_mhad" class="image"><img
                    src="images/projects/bajcsy/mhad.jpg" alt="Berkeley Multimodal Human Action Database (MHAD)"
                    /><span>Berkeley Multimodal Human Action Database (MHAD)</span></a>
                </div>
              </div>
              <p>
              The Berkeley Multimodal Human Action Database (MHAD) contains 11 actions
performed by 7 male and 5 female subjects in the range 23-30 years of age except
for one elderly subject. All the subjects performed 5 repetitions of each
action, yielding about 660 action sequences which correspond to about 82 minutes
of total recording time. In addition, we have recorded a T-pose for each subject
which can be used for the skeleton extraction; and the background data (with and
without the chair used in some of the activities). Figure 1 shows the snapshots
from all the actions taken by the front-facing camera and the corresponding
point clouds extracted from the Kinect data. The specified set of actions
comprises of the following: (1) actions with movement in both upper and lower
extremities, e.g., jumping in place, jumping jacks, throwing, etc., (2) actions
with high dynamics in upper extremities, e.g., waving hands, clapping hands,
etc. and (3) actions with high dynamics in lower extremities, e.g., sit down,
stand up. Prior to each recording, the subjects were given instructions on what
action to perform; however no specific details were given on how the action
should be executed (i.e., performance style or speed). The subjects have thus
incorporated different styles in performing some of the actions (e.g., punching,
throwing). Figure 2 shows a snapshot of the throwing action from the reference
camera of each camera cluster and from the two Kinect cameras. The figure
demonstrates the amount of information that can be obtained from multi-view and
depth observations as compared to a single viewpoint.
<a href="http://tele-immersion.citris-uc.org/berkeley_mhad">Details</a>
              </p>
            </div>

            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://decaf.berkeleyvision.org/" class="image"><img
                    src="images/projects/darrell/decaf.png" alt="DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition"
                    /><span>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</span></a>
                </div>
              </div>
              <p>
              We evaluate whether features extracted from the activation of a deep
convolutional network trained in a fully supervised fashion on a large, fixed
set of object recognition tasks can be repurposed to novel generic tasks. Our
generic tasks may differ significantly from the originally trained tasks and
there may be insufficient labeled or unlabeled data to conventionally train or
adapt a deep architecture to the new tasks. We investigate and visualize the
semantic clustering of deep convolutional features with respect to a variety
of such tasks, including scene recognition, domain adaptation, and fine-grained
recognition challenges. We compare the efficacy of relying on various network
levels to define a fixed feature, and report novel results that significantly
outperform the state-of-the-art on several important vision challenges. We are
releasing DeCAF, an open-source implementation of these deep convolutional
activation features, along with all associated network parameters to enable
vision researchers to be able to conduct experimentation with deep
representations across a range of visual concept learning paradigms.
              <a href="http://decaf.berkeleyvision.org/">Details</a> /
              <a href="https://github.com/UCB-ICSI-Vision-Group/decaf-release/">Code</a>
              </p>
            </div>
          </div>

          <div class="row">
            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://graphics.cs.cmu.edu/projects/whatMakesParis/" class="image"><img
                    src="images/projects/efros/paris.jpg" alt="What Makes Paris Look Like Paris?"
                    /><span>What Makes Paris Look Like Paris?</span></a>
                </div>
              </div>
              <p>
              Given a large repository of geotagged imagery, we seek to automatically find
visual elements, e.g. windows, balconies, and street signs, that are most
distinctive for a certain geo-spatial area, for example the city of Paris. This
is a tremendously difficult task as the visual features distinguishing
architectural elements of different places can be very subtle. In addition, we
face a hard search problem: given all possible patches in all images, which of
them are both frequently occurring and geographically informative? To address
these issues, we propose to use a discriminative clustering approach able to
take into account the weak geographic supervision. We show that geographically
representative image elements can be discovered automatically from Google Street
View imagery in a discriminative manner. We demonstrate that these elements are
visually interpretable and perceptually geo-informative. The discovered visual
elements can also support a variety of computational geography tasks, such as
mapping architectural correspondences and influences within and across cities,
finding representative elements at different geo-spatial scales, and
geographically-informed image retrieval.
<a href="http://graphics.cs.cmu.edu/projects/whatMakesParis/">Details</a>
              </p>
            </div>

            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="https://amplab.cs.berkeley.edu/publication/mli-an-api-for-distributed-machine-learning/" class="image"><img
                    src="images/projects/jordan/mlbase.png" alt="MLI: An API for Distributed Machine Learning"
                    /><span>MLI: An API for Distributed Machine Learning</span></a>
                </div>
              </div>
              <p>
              MLI is an Application Programming Interface designed to address the challenges
of building Machine Learning algorithms in a distributed setting based on
data-centric computing. Its primary goal is to simplify the development of
high-performance, scalable, distributed algorithms. Our initial results show
that, relative to existing systems, this interface can be used to build
distributed implementations of a wide variety of common Machine Learning
algorithms with minimal complexity and highly competitive performance and
scalability.
<a href="https://amplab.cs.berkeley.edu/publication/mli-an-api-for-distributed-machine-learning/">Details</a>
              </p>
            </div>
          </div>

          <div class="row">
            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://www.cs.berkeley.edu/~barron/SIRFS_release1.2.zip" class="image"><img
                    src="images/projects/malik/sirfs-model.png" alt="Shape, Illumination, and Reflectance from Shading"
                    /><span>Shape, Illumination, and Reflectance from Shading</span></a>
                </div>
              </div>
              <p>
              A fundamental problem in computer vision is that of inferring the intrinsic, 3D
structure of the world from flat, 2D images of that world. Traditional methods
for recovering scene properties such as shape, reflectance, or illumination rely
on multiple observations of the same scene to overconstrain the problem.
Recovering these same properties from a single image seems almost impossible in
comparison — there are an infinite number of shapes, paint, and lights that
exactly reproduce a single image. However, certain explanations are more likely
than others: surfaces tend to be smooth, paint tends to be uniform, and
illumination tends to be natural. We therefore pose this problem as one of
statistical inference, and define an optimization problem that searches for the
most likely explanation of a single image. Our technique can be viewed as a
superset of several classic computer vision problems (shape-from-shading,
intrinsic images, color constancy, illumination estimation, etc) and outperforms
all previous solutions to those constituent problems.
<a href="http://www.cs.berkeley.edu/~barron/">1st Author</a> /
<a href="http://www.cs.berkeley.edu/~barron/SIRFS_release1.2.zip">Code</a>
              </p>
            </div>

            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://www.cs.berkeley.edu/~ravir/reconstruct_EG.pdf" class="image"><img
                    src="images/projects/ramamoorthi/sharpening.png" alt="Sharpening Out of Focus Images using High-Frequency Transfer"
                    /><span>Sharpening Out of Focus Images using High-Frequency Transfer</span></a>
                </div>
              </div>
              <p>
              We propose a new method to sharpen out-of-focus images, that uses a similar but
different assisting sharp image provided by the user (such as multiple images of
the same subject in different positions captured using a burst of photographs).
We demonstrate sharpened results on out-of-focus images in macro, sports,
portrait and wildlife photography.
<a href="http://www.cs.berkeley.edu/~ravir/reconstruct_EG.pdf">Paper</a>
              </p>
            </div>
          </div>

          <div class="row">
            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://www-video.eecs.berkeley.edu/~avz/3d/3d.html" class="image"><img
                    src="images/projects/zakhor/3dcity.jpg" alt="Fast 3D City Model Generation"
                    /><span>Fast 3D City Model Generation</span></a>
                </div>
              </div>
              <p>
              Fast, automated generation of photo realistic 3D models of city environments for
the purpose of simulations and interactive walk-, drive-, or fly-thrus. This
goal requires the combination of techniques from various research areas.
<a href="http://www-video.eecs.berkeley.edu/~avz/3d/3d.html">Details</a>
              </p>
            </div>

            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://rll.berkeley.edu/trajopt/ijrr/" class="image"><img
                    src="images/projects/abbeel/2-TrajOpt.jpg" alt="High-degree-of-freedom motion planning"
                    /><span>High-degree-of-freedom motion planning</span></a>
                </div>
              </div>
              <p>
              We are developing a new optimization-based approach for robotic motion planning
among obstacles. Our algorithm can be used to ﬁnd collision-free trajectories
from naive, straight-line initializations that might be in collision. At the
core of our approach are (i) A sequential convex optimization procedure, which
penalizes collisions with a hinge loss and increases the penalty coefﬁcients in
an outer loop as necessary. (ii) An efﬁcient formulation of the no-collisions
constraint that directly considers continuous-time safety. Our algorithm is
implemented in a software package called TrajOpt. We have experimental results
that compare TrajOpt with CHOMP and randomized planners from OMPL, with regard
to planning time and path quality. We have considered motion planning for 7 DOF
robot arms, 18 DOF full-body robots, statically stable walking motion for the 34
DOF Atlas humanoid robot, and physical experiments with the 18 DOF PR2. We have
also applied TrajOpt to plan curvature-constrained steerable needle trajectories
in the SE(3) conﬁguration space and multiple non-intersecting curved channels
within 3D-printed implants for intracavitary brachytherapy.
<a href="http://rll.berkeley.edu/trajopt/ijrr/">Details</a> /
<a href="http://rll.berkeley.edu/trajopt/">Code</a>
              </p>
            </div>
          </div>

          <div class="row">
            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://vis.berkeley.edu/papers/deanim/" class="image"><img
                    src="images/projects/agrawala/deanimation.jpg" alt="Selectively De-Animating Video"
                    /><span>Selectively De-Animating Video</span></a>
                </div>
              </div>
              <p>
              We present a semi-automated technique for selectively de-animating video to
remove the large-scale motions of one or more objects so that other motions are
easier to see. The user draws strokes to indicate the regions of the video that
should be immobilized, and our algorithm warps the video to remove the
large-scale motion of these regions while leaving finer-scale, relative motions
intact. However, such warps may introduce unnatural motions in previously
motionless areas, such as background regions. We therefore use a graph-cut-based
optimization to composite the warped video regions with still frames from the
input video; we also optionally loop the output in a seamless manner. Our
technique enables a number of applications such as clearer motion visualization,
simpler creation of artistic cinemagraphs (photos that include looping motions
in some regions), and new ways to edit appearance and complicated motion paths
in video by manipulating a de-animated representation. We demonstrate the
success of our technique with a number of motion visualizations, cinemagraphs
and video editing examples created from a variety of short input videos, as well
as visual and numerical comparison to previous techniques.
<a href="http://vis.berkeley.edu/papers/deanim/">Details</a>
              </p>
            </div>

            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://tele-immersion.citris-uc.org/telemedicine" class="image"><img
                    src="images/projects/bajcsy/rehabilitation.jpg" alt="Tele-Rehabilitation"
                    /><span>Tele-Rehabilitation</span></a>
                </div>
              </div>
              <p>
              As we have demonstrated in our past research experiments of teaching TaiChi
using teleimmersive feedback, users are able to learn faster and better with
this technology. In collaboration with Dr. Jay Han, University of California at
Davis Medical Center (UCDMC) we are building a framework for
tele-rehabilitation. Our focus is accurate and reliable assessments of upper
extremity which is critical because functionality of upper extremity affects
essentially all activities of daily living, independence, and overall quality of
life. In this application we have a remote health-coach who is instructing the
patient during his/her exercises through the teleimmersion technology. From the
stereo data, we extract the skeleton representation for two reasons: one is to
reduce the size of data transmission which may be important for rural clinics
with limited bandwidth availability and second are privacy reasons. We are
developing markerless body-tracking algorithm and workspace analysis for upper
extremity from the 3D video data sources. The patient gets the visual feedback
to correct his/her performance while various tasks in the virtual environment
can be design to elevate the attention of the patient. With respect to the
proposed teleimmersive framework, we will research integration of different
sensors with visual data for real-time feedback within a telemedicine model. We
are also interested in modeling of the coach and student with respect to body
dynamics and movement prediction and to provide appropriate feedback on the
performance to the patient. The research was supported through 2011 by CITRIS
Seed Grant.
<a href="http://tele-immersion.citris-uc.org/telemedicine">Details</a>
              </p>
            </div>
          </div>

          <div class="row">
            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://www.cs.berkeley.edu/~nzhang/" class="image"><img
                    src="images/projects/darrell/dpd.png" alt="Deformable Part Descriptors for Fine-grained Recognition and Attribute Prediction"
                    /><span>Deformable Part Descriptors for Fine-grained Recognition and Attribute Prediction</span></a>
                </div>
              </div>
              <p>
              Recognizing objects in fine-grained domains can be extremely challenging due to
the subtle differences between subcategories. Discriminative markings are
often highly localized, leading traditional object recognition approaches to
struggle with the large pose variation often present in these domains.
Pose-normalization seeks to align training exemplars, either piecewise by part
or globally for the whole object, effectively factoring out differences in pose
and in viewing angle. Prior approaches relied on computationally-expensive
filter ensembles for part localization and required extensive supervision. This
paper proposes two pose-normalized descriptors based on
computationally-efficient deformable part models. The first leverages the
semantics inherent in strongly-supervised DPM parts. The second exploits weak
semantic annotations to learn cross-component correspondences, computing
pose-normalized descriptors from the latent parts of a weakly-supervised DPM.
These representations enable pooling across pose and viewpoint, in turn
facilitating tasks such as fine-grained recognition and attribute prediction.
Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human
Attribute dataset demonstrate significant improvements over state-of-art
algorithms.
<a href="http://www.cs.berkeley.edu/~nzhang/">1st Author</a>
              </p>
            </div>

            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://graphics.cs.cmu.edu/projects/discriminativePatches/" class="image"><img
                    src="images/projects/efros/midlevel.jpg" alt="Unsupervised Discovery of Mid-Level Discriminative Patches"
                    /><span>Unsupervised Discovery of Mid-Level Discriminative Patches</span></a>
                </div>
              </div>
              <p>
              The goal of this paper is to discover a set of discriminative patches which can
serve as a fully unsupervised mid-level visual representation. The desired
patches need to satisfy two requirements: 1) to be representative, they need to
occur frequently enough in the visual world; 2) to be discriminative, they need
to be different enough from the rest of the visual world. The patches could
correspond to parts, objects, “visual phrases”, etc. but are not restricted to
be any one of them. We pose this as an unsupervised discriminative clustering
problem on a huge dataset of image patches. We use an iterative procedure which
alternates between clustering and training discriminative classifiers, while
applying careful cross-validation at each step to prevent overfitting. The paper
experimentally demonstrates the effectiveness of discriminative patches as an
unsupervised mid-level visual representation, suggesting that it could be used
in place of visual words for many tasks. Furthermore, discrim- inative patches
can also be used in a supervised regime, such as scene classification, where
they demonstrate state-of-the-art performance on the MIT Indoor-67 dataset.
<a href="http://graphics.cs.cmu.edu/projects/discriminativePatches/">Details</a>
              </p>
            </div>
          </div>

          <div class="row">
            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://www.stat.berkeley.edu/~tab/" class="image">
                    <img src="images/projects/jordan/bayes.png" alt="Streaming
                    variational Bayes"/>
                    <span>Streaming variational Bayes</span></a>
                </div>
              </div>
              <p>
              We present SDA-Bayes, a framework for (S)treaming, (D)istributed, (A)synchronous
computation of a Bayesian posterior. The framework makes streaming updates to
the estimated posterior according to a user-specified approximation primitive
function. We demonstrate the usefulness of our framework, with variational Bayes
(VB) as the primitive, by fitting the latent Dirichlet allocation model to two
large-scale document collections. We demonstrate the advantages of our algorithm
over stochastic variational inference (SVI), both in the single-pass setting SVI
was designed for and in the streaming setting, to which SVI does not apply.
<a href="http://www.stat.berkeley.edu/~tab/">1st Author</a>
              </p>
            </div>

            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="https://github.com/s-gupta/rgbd" class="image"><img
                    src="images/projects/malik/rgbdseg.png" alt="Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images"
                    /><span>Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</span></a>
                </div>
              </div>
              <p>
              We address the problems of contour detection, bottom-up grouping and semantic
segmentation using RGB-D data. We focus on the challenging setting of cluttered
indoor scenes, and evaluate our approach on the recently introduced NYU-Depth
V2 (NYUD2) dataset. We propose algorithms for object boundary detection and
hierarchical segmentation that generalize the gPb−ucm approach by
making effective use of depth information. We show that our system can label
each contour with its type (depth, normal or albedo). We also propose a generic
method for long-range amodal completion of surfaces and show its effectiveness
in grouping. We then turn to the problem of semantic segmentation and propose
a simple approach that classifies superpixels into the 40 dominant object
categories in NYUD2. We use both generic and class-specific features to encode
the appearance and geometry of objects. We also show how our approach can be
used for scene classification, and how this contextual information in turn
improves object recognition. In all of these tasks, we report significant
improvements over the state-of-the-art.
<a href="http://www.cs.berkeley.edu/~barron/">1st Author</a>
<a href="https://github.com/s-gupta/rgbd">Code</a>
              </p>
            </div>
          </div>

          <div class="row">
            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://graphics.berkeley.edu/papers/Reddy-FSD-2012-10/index.html" class="image"><img
                    src="images/projects/ramamoorthi/frequencyspace.jpg" alt="Frequency-Space Decomposition and Acquisition of Light Transport under Spatially Varying Illumination"
                    /><span>Frequency-Space Decomposition and Acquisition of Light Transport under Spatially Varying Illumination</span></a>
                </div>
              </div>
              <p>
              We show that, under spatially varying illumination, the light transport of
diffuse scenes can be decomposed into direct, near-range (subsurface scattering
and local inter-reflections) and far-range transports (diffuse
inter-reflections). We show that these three component transports are redundant
either in the spatial or the frequency domain and can be separated using
appropriate illumination patterns. We propose a novel, efficient method to
sequentially separate and acquire the component transports. First, we acquire
the direct transport by extending the direct-global separation technique from
floodlit images to full transport matrices. Next, we separate and acquire the
near-range transport by illuminating patterns sampled uniformly in the frequency
domain. Finally, we acquire the far-range transport by illuminating
low-frequency patterns. We show that theoretically, our acquisition method
achieves the lower bound our model places on the required number of patterns. We
quantify the savings in number of patterns over the brute force approach. We
validate our observations and acquisition method with rendered and real examples
throughout.
<a href="http://graphics.berkeley.edu/papers/Reddy-FSD-2012-10/index.html">Details</a>
              </p>
            </div>

            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://www-video.eecs.berkeley.edu/~avz/classify.html" class="image"><img
                    src="images/projects/zakhor/urbanclassify.gif" alt="Classifying Urban Landscape In Aerial Lidar"
                    /><span>Classifying Urban Landscape In Aerial Lidar</span></a>
                </div>
              </div>
              <p>
              The classification of urban landscape in aerial LiDAR point clouds can
potentially improve the quality of largescale 3D urban models, as well as
increase the breadth of objects that can be detected and recognized in urban
environments. In this paper, we introduce a multi-category classification system
for aerial LiDAR point clouds. We propose the use of a cascade of binary
classifiers for labeling each LiDAR return of an input point cloud as one of
five categories: water, ground, roof, tree, and other. Each binary classifier
identifies LiDAR returns corresponding to a particular class, and removes them
from the processing pipeline. Categories of LiDAR returns that exhibit the most
discriminating features, such as water and ground, are identified first. More
complex categories, such as trees, are identified later in the pipeline after
contextual information, such as the location of ground and roofs, has been
obtained, and a significant number of LiDAR returns have already been removed
from the pipeline. We demonstrate results on a North American dataset,
consisting of 125 million LiDAR returns over 3 km2, and a European dataset,
consisting of 200 million LiDAR returns over 7 km2. We show that our ground,
roof, and tree classifiers, when trained on one dataset, perform accurately on
the other dataset.
<a href="http://www-video.eecs.berkeley.edu/~avz/classify.html">Details</a>
              </p>
            </div>
          </div>

          <div class="row">
            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://rll.berkeley.edu/beliefopt/" class="image"><img
                    src="images/projects/abbeel/3-belief-space-planning.png" alt="Gaussian belief space planning"
                    /><span>Gaussian belief space planning</span></a>
                </div>
              </div>
              <p>
              In many home and service applications, an emerging class of articulated robots
such as the Raven and Baxter trade off precision in actuation and sensing to
reduce costs and to reduce the potential for injury to humans in their
workspaces. For planning and control of such robots, planning in belief space,
i.e., modeling such problems as POMDPs, has shown great promise but existing
belief space planning methods have primarily been applied to cases where robots
can be approximated as points or spheres. We have extended the belief space
framework to treat articulated robots where the linkage can be decomposed into
convex components. To allow planning and collision avoidance in Gaussian belief
spaces, we introduce the concept of sigma hulls: convex hulls of robot links
transformed according to the sigma standard deviation boundary points generated
by the Unscented Kalman ﬁlter (UKF). We characterize the signed distances
between sigma hulls and obstacles in the workspace to formulate efﬁcient
collision avoidance constraints within an optimization-based planning framework.
We have promising preliminary results in simulation for planning motions for a
4-DOF planar robot and a 7-DOF articulated robot with imprecise actuation and
inaccurate sensors. These experiments suggest that the sigma hull framework can
signiﬁcantly reduce the probability of collision and is computationally efﬁcient
enough to permit iterative re-planning for model predictive control.  We have
also considered discontinuities in sensing domains are common when planning for
many robotic navigation and manipulation tasks. For cameras and 3D sensors,
discontinuities may be inherent in sensor ﬁeld of view or may change over time
due to occlusions that are created by moving obstructions and movements of the
sensor. The associated gaps in sensor information due to missing measurements
pose a challenge for belief space and related optimization-based planning
methods since there is no gradient information when the system state is outside
the sensing domain. We address this in a belief space context by considering the
signed distance to the sensing region.
<a href="http://rll.berkeley.edu/beliefopt/">Details</a> /
<a href="https://github.com/dementrock/trajopt/tree/bsp_collision">Code</a>
              </p>
            </div>

            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://vis.berkeley.edu/papers/autocinema/" class="image"><img
                    src="images/projects/agrawala/autocinema.jpg" alt="Automatic Cinemagraph Portraits"
                    /><span>Automatic Cinemagraph Portraits</span></a>
                </div>
              </div>
              <p>
              Cinemagraphs are a popular new type of visual media that lie in-between photos
and video; some parts of the frame are animated and loop seamlessly, while other
parts of the frame remain completely still. Cinemagraphs are especially
effective for portraits because they capture the nuances of our dynamic facial
expressions. We present a completely automatic algorithm for generating portrait
cinemagraphs from a short video captured with a hand-held camera. Our algorithm
uses a combination of face tracking and point tracking to segment face motions
into two classes: gross, large-scale motions that should be removed from the
video, and dynamic facial expressions that should be preserved. This
segmentation informs a spatially-varying warp that removes the large-scale
motion, and a graph-cut segmentation of the frame into dynamic and still regions
that preserves the finer-scale facial expression motions. We demonstrate the
success of our method with a variety of results and a comparison to previous
work.
<a href="http://vis.berkeley.edu/papers/autocinema/">Details</a>
              </p>
            </div>
          </div>

          <div class="row">
            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://tele-immersion.citris-uc.org/stereo" class="image"><img
                    src="images/projects/bajcsy/stereo.png" alt="Realtime Stereo Reconstruction and Visualization"
                    /><span>Realtime Stereo Reconstruction and Visualization</span></a>
                </div>
              </div>
              <p>
              In the recent years we have developed a real-time CPU-based stereo
reconstruction algorithm which reconstructs depth information from two camera
views. The general idea of the algorithm is to perform accurate and efficient
stereo computation of the scene by employing fast stereo matching through an
adaptive meshing scheme. The mesh is created based on the detected variance of
different image regions. Special properties of the mesh used in our encoding,
allows for fast and robust implementation of post-processing algorithms. Our
mesh-based algorithm also encodes the depth information with high compression
ratio to minimize the size of packets sent over the network.
<a href="http://tele-immersion.citris-uc.org/stereo">Details</a>
              </p>
            </div>

            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="http://www.eecs.berkeley.edu/~jiayq/"
                    class="image"><img
                    src="images/projects/darrell/concept-learn-curves.png"
                    alt="Visual Concept Learning" /><span>Visual Concept
                      Learning</span></a>
                </div>
              </div>
              <p>
How can we learn visual concepts from strong-sampled, positive-only data as
humans do.  We present a joint framework with vision and cogscience with: 1. a
visually-grounded Bayesian Concept Model, 2. visual features from large-scale
data, and 3. hierarchically nested concept space.
<a href="http://www.eecs.berkeley.edu/~jiayq/">1st Author</a>
              </p>
            </div>
          </div>

        </section>
      </div>


<!--

          <div class="row">
            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="surl" class="image"><img
                    src="images/projects/sim" alt="sname"
                    /><span>sname</span></a>
                </div>
              </div>
              <p>
              sabstract
<a href="surl">Details</a>
              </p>
            </div>

            <div class="6u">
              <div class="gallery">
                <div>
                  <a href="surl" class="image"><img
                    src="images/projects/sim" alt="sname"
                    /><span>sname</span></a>
                </div>
              </div>
              <p>
              sabstract
<a href="surl">Details</a>
              </p>
            </div>
          </div>
          -->
        </section>
      </div>

    <!-- Footer Wrapper -->
      <div id="footer-wrapper">

        <!-- Footer -->
          <div id="footer" class="container">
            <header>
              <h2>Questions or comments? <strong>Contact us:</strong></h2>
            </header>
            <div class="row">
              <div class="-3u 6u">
                <section>
                <p>
                  For more information, contact the BVLC Director, Prof. Trevor
                  Darrell:
                  <br>
                  <a href="mailto:trevor@cs.berkeley.edu">trevor [AT]
                    cs.berkeley.edu</a>.
                </p>
                  <ul class="actions">
                    <li><a class="button button-icon icon icon-envelope"
                      href="mailto:trevor@cs.berkeley.edu">Contact BVLC</a></li>
                  </ul>
                </section>
              </div>
            </div>
          </div>

        <!-- Copyright -->
          <div id="copyright" class="container">
            <ul class="links">
              <li>&copy; BVLC 2013.</li>
              <li>Design based on: <a href="http://html5up.net/">HTML5 UP</a></li>
            </ul>
          </div>

      </div>

  </body>
</html>
